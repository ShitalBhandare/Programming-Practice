

OS Basics:


OS?
- It is the interface between the user and the computer hardware.

Types of Operating System (OS):
Batch OS: 
– A set of similar jobs are stored in the main memory for execution. 
- A job gets assigned to the CPU, only when the execution of the previous job completes.

Multiprogramming OS:
- OS runs one job1 and when job1 needs some resources, it starts another job2
- This way, the CPU is never kept idle and the user gets the flavor of getting multiple tasks done at once.


Multitasking OS 
– Multitasking OS combines the benefits of Multiprogramming OS and CPU scheduling to perform quick switches between jobs. 
- The switch is so quick that the user can interact with each program as it runs

Time Sharing OS 
– Time-sharing systems require interaction with the user to instruct the OS to perform various tasks

Real Time OS 
– Real-Time OS are usually built for dedicated systems to accomplish a specific set of tasks within deadlines.



Threads:
- A thread is a lightweight process and forms the basic unit of CPU utilization.
- A thread has its own program counter, register set, and stack
- A thread shares resources with other threads of the same process the code section, the data section, files and signals.
- can be created using fork system call

	User level thread 								Kernel level thread
User threads are implemented by users. 					kernel threads are implemented by OS.
OS doesn’t recognize user level threads. 				Kernel threads are recognized by OS.
Implementation of User threads is easy. 				Implementation of Kernel thread is complicated.
Context switch time is less. 						Context switch time is more.
Context switch requires no hardware support. 				Hardware support is needed.
If one user level thread performs blocking operation  			If one kernel thread performs blocking operation then another 	 thread can continue execution.
then entire process will be blocked.



Process:
- A process is a instance of program under execution. 
- The value of program counter (PC) indicates the address of the next instruction of the process being executed. 
- Each process is represented by a Process Control Block (PCB).
- Turn Around Time = Completion Time - Arrival Time 
- Waiting Time = Turn Around Time - Burst Time 


Scheduling:
- In multiprogramming systems, one process can use CPU while another is waiting for I/O. This is possible only with process scheduling.
- Objectives of Process Scheduling Algorithm:

	Max CPU utilization (Keep CPU as busy as possible)
    Fair allocation of CPU.
    Max throughput (Number of processes that complete their execution per time unit)
    Min turnaround time (Time taken by a process to finish execution)
    Min waiting time (Time for which a process waits in ready queue)
    Min response time (Time when a process produces first response)



Scheduling Algorithms:
FCFS
- is a non-preemptive scheduling algorithm.
- Suffers from Convoy effect
- Hence in Convoy Effect, one slow process slows down the performance of the entire set of processes, and leads to wastage of CPU time and other devices.

Shortest Job First (SJF) 
- Preemptive in nature 
- Good avg waiting time
- Process may suffer from starvation


Longest Job First (LJF):
- Similar to SJF, but longest process gives time first to execute
- Non-preemptive in nature

Shortest Remaining Time First (SRTF): 
- It is preemptive mode of SJF algorithm in which jobs are schedule according to shortest remaining time.


Longest Remaining Time First (LRTF): 
- It is preemptive mode of LJF algorithm in which we give priority to the process having largest burst time remaining.
- It is simple, easy to implement, and starvation-free as all processes get fair share of CPU.
- Preemptive in nature
- more overhead of context switching.


Round Robin Scheduling: 
- Each process is assigned a fixed time(Time Quantum/Time Slice) in cyclic way.
- It is designed especially for the time-sharing system. 


Priority Based scheduling (Non-Preemptive): 
- In this scheduling, processes are scheduled according to their priorities, i.e., highest priority process is scheduled first. 
- If priorities of two processes match, then schedule according to arrival time. 
- Here starvation of process is possible.


Highest Response Ratio Next (HRRN): 
- In this scheduling, processes with highest response ratio is scheduled. This algorithm avoids starvation.
- Response Ratio = (Waiting Time + Burst time) / Burst time


Multilevel Queue Scheduling (MLQ): 
- According to the priority of process, processes are placed in the different queues. 
- Generally high priority process are placed in the top level queue. 
- Only after completion of processes from top level queue, lower level queued processes are scheduled. 


Multi level Feedback Queue (MLFQ) Scheduling: 
- It allows the process to move in between queues. 
- The idea is to separate processes according to the characteristics of their CPU bursts. 
- If a process uses too much CPU time, it is moved to a lower-priority queue. 


Some useful facts about Scheduling Algorithms:

    FCFS can cause long waiting times, especially when the first job takes too much CPU time.
    Both SJF and Shortest Remaining time first algorithms may cause starvation. Consider a situation when a long process is there in the ready queue and shorter processes keep coming.
    If time quantum for Round Robin scheduling is very large, then it behaves same as FCFS scheduling.
    SJF is optimal in terms of average waiting time for a given set of processes. SJF gives minimum average waiting time, but problems with SJF is how to know/predict the time of next job
	
	
Race around Condition 
– The final output of the code depends on the order in which the variables are accessed. This is termed as the race around condition. 


Virtual Memory:
- Creates an illusion that each user has one or more contiguous address spaces, each beginning at address zero. 
- The sizes of such virtual address spaces is generally very high.

Thrashing?
- Thrashing is a situation when the performance of a computer degrades or collapses.
- Thrashing occurs when a system spends more time processing page faults than executing transactions.

Belady’s Anomaly?
- Bélády’s anomaly is an anomaly with some page replacement policies where increasing the number of page frames results in an increase in the number of page faults. 
- It occurs with First in First Out page replacement is used.

Mutex VS Semaphore?
- mutex and semaphore are kernel resources that provide synchronization services (also called as synchronization primitives).
- Atomicity: Either complete transaction can happen or nothing. Operation should not be divisible.
- Strictly speaking, a mutex is locking mechanism used to synchronize access to a resource. 
- Only one task (can be a thread or process based on OS abstraction) can acquire the mutex. 
- Semaphore is signaling mechanism (“I am done, you can carry on” kind of signal). 
- For example, if you are listening songs (assume it as one task) on your mobile and at the same time your friend calls you, an interrupt is triggered 
  upon which an interrupt service routine (ISR) signals the call processing task to wakeup.


Synchronization Tools:
A Semaphore is an integer variable that is accessed only through two atomic operations, wait () and signal (). 
An atomic operation is executed in a single CPU time slice without any pre-emption. Semaphores are of two types:

    Counting Semaphore – A counting semaphore is an integer variable whose value can range over an unrestricted domain.
    Mutex – Binary Semaphores are called Mutex. These can have only two values, 0 or 1. 
		  - The operations wait () and signal () operate on these in a similar fashion.




Critical Section?
- In simple terms a critical section is group of instructions/statements or region of code that need to be executed atomically (read this post for atomicity), 
  such as accessing a resource (file, input or output port, global data, etc.).
- Parts of Critical Section
	Entry Section
	Critical Section
	Exit Section
	Remainder Section
- Simple Solution for resolving Critical Section:
	acquireLock();
	Process Critical Section
	releaseLock();
	
Solution to critical section involves below things:

    Mutual Exclusion – If a process Pi is executing in its critical section, then no other process is allowed to enter into the critical section.
    Progress – If no process is executing in the critical section, then the decision of a process to enter a critical section cannot be made by any other process that is executing in its remainder section. The selection of the process cannot be postponed indefinitely.
    Bounded Waiting – There exists a bound on the number of times other processes can enter into the critical section after a process has made request to access the critical section and before the requested is granted.

	
	
	
Deadlock:
- Deadlock is a situation where a set of processes are blocked because each process is holding a resource and waiting for another resource acquired by some other process.
	
Deadlock can arise if following four conditions hold simultaneously (Necessary Conditions)
- Mutual Exclusion: One or more than one resource are non-sharable (Only one process can use at a time)
- Hold and Wait: A process is holding at least one resource and waiting for resources.
- No Preemption: A resource cannot be taken from a process unless the process releases the resource.
- Circular Wait: A set of processes are waiting for each other in circular form.


Methods for handling deadlock: There are three ways to handle deadlock

- Deadlock prevention or avoidance: The idea is to not let the system into deadlock state.
- Deadlock detection and recovery : Let deadlock occur, then do preemption to handle it once occurred.
- Ignore the problem all together – : If deadlock is very rare, then let it happen and reboot the system. This is the approach that both Windows and UNIX take.


Deadlock Prevention

	- It is important to prevent a deadlock before it can occur. So, the system checks each transaction before it is executed to make sure it does not lead to deadlock. If there is even a slight possibility that a transaction may lead to deadlock, it is never allowed to execute.

	- Some deadlock prevention schemes that use timestamps in order to make sure that a deadlock does not occur are given as follows:

    1) Wait - Die Scheme

    	In the wait - die scheme, if a transaction T1 requests for a resource that is held by transaction T2, one of the following two scenarios may occur:
        TS(T1) < TS(T2) - If T1 is older than T2 i.e T1 came in the system earlier than T2, then it is allowed to wait for the resource which will be free when T2 has completed its execution.
        TS(T1) > TS(T2) - If T1 is younger than T2 i.e T1 came in the system after T2, then T1 is killed. It is restarted later with the same timestamp.
    
    2) Wound - Wait Scheme

    	In the wound - wait scheme, if a transaction T1 requests for a resource that is held by transaction T2, one of the following two possibilities may occur:
        TS(T1) < TS(T2) - If T1 is older than T2 i.e T1 came in the system earlier than T2, then it is allowed to roll back T2 or wound T2. Then T1 takes the resource and completes its execution. T2 is later restarted with the same timestamp.
        TS(T1) > TS(T2) - If T1 is younger than T2 i.e T1 came in the system after T2, then it is allowed to wait for the resource which will be free when T2 has completed its execution.

Deadlock Avoidance

	- It is better to avoid a deadlock rather than take measures after the deadlock has occurred. The wait for graph can be used for deadlock avoidance. This is however only useful for smaller databases as it can get quite complex in larger databases.
Wait for graph

	- The wait for graph shows the relationship between the resources and transactions. If a transaction requests a resource or if it already holds a resource, it is visible as an edge on the wait for graph. If the wait for graph contains a cycle, then there may be a deadlock in the system, otherwise not.
Wait for graph


Banker’s Algorithm

- Bankers’s Algorithm is resource allocation and deadlock avoidance algorithm which test all the request made by processes for resources, 
it checks for the safe state, if after granting request system remains in the safe state it allows the request and if there is no safe state it doesn’t allow the request made by the process.
it checks for the safe state, if after granting request system remains in the safe state it allows the request and if there is no safe state it doesn’t allow the request made by the process.


	
Memory Management:
These techniques allow the memory to be shared among multiple processes.

    Overlays – The memory should contain only those instructions and data that are required at a given time.
    Swapping – In multiprogramming, the instructions that have used the time slice are swapped out from the memory.

Memory Management Techniques:

(a) Single Partition Allocation Schemes –
The memory is divided into two parts. One part is kept to be used by the OS and the other is kept to be used by the users.

(b) Multiple Partition Schemes –

    Fixed Partition – The memory is divided into fixed size partitions.
    Variable Partition – The memory is divided into variable sized partitions.

Variable partition allocation schemes:

    First Fit – The arriving process is allotted the first hole of memory in which it fits completely.
    Best Fit – The arriving process is allotted the hole of memory in which it fits the best by leaving the minimum memory empty.
    Worst Fit – The arriving process is allotted the hole of memory in which it leaves the maximum gap.
	

    Paging –
    The physical memory is divided into equal sized frames. The main memory is divided into fixed size pages. The size of a physical memory frame is equal to the size of a virtual memory frame.
    Segmentation –
    Segmentation is implemented to give users view of memory. The logical address space is a collection of segments. Segmentation can be implemented with or without the use of paging.

Page Fault:
A page fault occurs when a program attempts to access data or code that is in its address space, but is not currently located in the system RAM.


Page Replacement Algorithms:

First In First Out (FIFO):
- This is the simplest page replacement algorithm. In this algorithm, operating system keeps track of all pages in the memory in a queue, oldest page is in the front of the queue. 
- When a page needs to be replaced page in the front of the queue is selected for removal.
- Optimal page replacement is perfect, but not possible in practice as an operating system cannot know future requests.

Least Recently Used (LRU)
– In this algorithm, the page will be replaced which is least recently used.


Optimal Page replacement –
In this algorithm, pages are replaced which are not used for the longest duration of time in the future. 


File System: A file is a collection of related information that is recorded on secondary storage. Or file is a collection of logically related entities.

File Directories: Collection of files is a file directory.
		SINGLE-LEVEL DIRECTORY
		Two-level Directory
		Tree-like directory

File Allocation Methods:

    Continuous Allocation: A single continuous set of blocks is allocated to a file at the time of file creation.
    Linked Allocation(Non-contiguous allocation): Allocation is on an individual block basis. Each block contains a pointer to the next block in the chain.
    Indexed Allocation : It addresses many of the problems of contiguous and chained allocation. In this case, the file allocation table contains a separate one-level index for each file




Disk Scheduling:
Disk scheduling is done by operating systems to schedule I/O requests arriving for disk. Disk scheduling is also known as I/O scheduling.

    Seek Time: Seek time is the time taken to locate the disk arm to a specified track where the data is to be read or write.
    Rotational Latency: Rotational Latency is the time taken by the desired sector of disk to rotate into a position so that it can access the read/write heads.
    Transfer Time: Transfer time is the time to transfer the data. It depends on the rotating speed of the disk and number of bytes to be transferred.
    Disk Access Time: Seek Time + Rotational Latency + Transfer Time
    Disk Response Time: Response Time is the average of time spent by a request waiting to perform its I/O operation. Average Response time is the response time of the all requests.

	
Disk Scheduling Algorithms:

FCFS: FCFS is the simplest of all the Disk Scheduling Algorithms. In FCFS, the requests are addressed in the order they arrive in the disk queue.
SSTF: In SSTF (Shortest Seek Time First), requests having shortest seek time are executed first. 
SCAN: In SCAN algorithm the disk arm moves into a particular direction and services the requests coming in its path and after reaching the end of the disk, it reverses its direction and again services the request arriving in its path. 
	So, this algorithm works like an elevator and hence also known as elevator algorithm. 
CSCAN: Circular scan. When reaches at the end of the disk, goes to other end of the disk.
LOOK: It is similar to the SCAN disk scheduling algorithm except for the difference that the disk arm in spite of going to the end of the disk goes only to the last request to be serviced in front of the head and then reverses its direction from there only. 
CLOOK: As LOOK is similar to SCAN algorithm, in a similar way, CLOOK is similar to CSCAN disk scheduling algorithm. In CLOOK, the disk arm in spite of going to the end goes only to the last request to be serviced in front of the head and then from there goes to the other end’s last request. 	
	


Inter Process Communication:


    Independent process.
    Co-operating process.

An independent process is not affected by the execution of other processes while a co-operating process can be affected by other executing processes. 

Processes can communicate with each other using these two ways:

    Shared Memory
    Message passing

	
	
	
References:
https://www.geeksforgeeks.org/last-minute-notes-operating-systems/
https://www.geeksforgeeks.org/commonly-asked-operating-systems-interview-questions-set-1/


